{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name:- Sudhanshu Vishwakarma\n",
    "### Batch:- B-2 Hons\n",
    "### Subject:- Computational Linguistics Lab Exam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**:- Download the above dataset and build a text classifier model that can predict the subject areas given paper abstracts and titles using latest NLP techniques taught to you. Show the use of word2vec and BERT especially here.Extra 1 hour is given to write comments in your code and upload it. Proper commenting after each function or wherever seems fit should be done. Upload the pdf of your code here. Plag should not be more than 10%.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries for word2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('arxiv_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Word2Vec Tokenizer function(available online) for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from collections import Counter\n",
    "\n",
    "class Word2VecTokenizer(Tokenizer):\n",
    "    def __init__(self, texts, vocab_size=5000, window_size=5):\n",
    "        super().__init__(num_words=vocab_size)\n",
    "        self.tokenizer = Tokenizer(num_words=vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.fit_on_texts(texts)\n",
    "        self.word2id = self.word_index\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.vocab = set(self.word2id.keys())\n",
    "        self.word_counts = Counter(self.word_counts)\n",
    "        \n",
    "        self.generate_pairs(texts)\n",
    "        \n",
    "    def generate_pairs(self, texts):\n",
    "        data = []\n",
    "        for text in texts:\n",
    "            seq = self.tokenizer.texts_to_sequences([str(text)])[0] # explicitly convert to string\n",
    "            pairs, _ = skipgrams(sequence=seq, vocabulary_size=self.vocab_size, window_size=self.window_size, negative_samples=0)\n",
    "            for pair in pairs:\n",
    "                data.append([pair[0], pair[1]])\n",
    "        self.data = np.array(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset and tuning to fit into the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:895: UserWarning: unknown class(es) ['j'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing the text data\n",
    "tokenizer = Word2VecTokenizer(train_data, 'summaries')\n",
    "tokenizer.fit_on_texts(train_data['summaries'])\n",
    "\n",
    "# Convert text data into sequence of integers\n",
    "X_train_seq = tokenizer.tokenizer.texts_to_sequences(train_data['summaries'])\n",
    "X_val_seq = tokenizer.tokenizer.texts_to_sequences(val_data['summaries'])\n",
    "\n",
    "# Pad the sequence with zeros to have a fixed length\n",
    "max_seq_len = 500\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_len, padding='post')\n",
    "X_val_padded = pad_sequences(X_val_seq, maxlen=max_seq_len, padding='post')\n",
    "\n",
    "# Convert labels to binary representation\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(train_data['terms'])\n",
    "y_val = mlb.transform(val_data['terms'])\n",
    "num_labels = len(mlb.classes_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model and running it for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 35/324 [==>...........................] - ETA: 43:36 - loss: 0.2929 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, 128, input_length=max_seq_len))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(num_labels, activation='sigmoid'))\n",
    "\n",
    "# Compiling the model for fitting the data into the model and running it\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model by fitting the preprocessed data\n",
    "model.fit(X_train_padded, y_train, batch_size=128, epochs=5, validation_data=(X_val_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model made based on classification report that is precision,recall and f1 score\n",
    "#also calculating the accuracy\n",
    "y_pred = model.predict(X_val_padded)\n",
    "y_pred_labels = mlb.inverse_transform(y_pred > 0.5)\n",
    "y_val_labels = mlb.inverse_transform(y_val)\n",
    "print(classification_report(y_val_labels, y_pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the model is running but will take atleast an hour to train so time boundation is there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT IMPLEMENTATION On this dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries for BERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the terms into singular terms rather than a set of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i, row in df.iterrows():\n",
    "    for term in row['terms']:\n",
    "        new_item = {\n",
    "            'titles': row['titles'],\n",
    "            'summaries': row['summaries'],\n",
    "            'terms': [str(term)],\n",
    "            'label': row['label']\n",
    "        }\n",
    "        new_data.append(new_item)\n",
    "\n",
    "new_df = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing BERT and making train_test_split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_abstract, X_val_abstract, X_train_title, X_val_title, y_train, y_val = train_test_split(\n",
    "    abstract_tensors, title_tensors, label_tensors, test_size=0.2, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
